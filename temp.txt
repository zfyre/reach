Response: <?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%28ti%3A%22Diffusion%20Models%22%29%20AND%20%28abs%3A%22Diffusion%20Models%22%29%20ANDNOT%20%28abs%3Aprostate%20AND%20abs%3Asurgical%20AND%20abs%3Agender%20AND%20abs%3Aethinic%20AND%20abs%3Aquadrotor%29%20AND%20%28cat%3Acs.AI%20OR%20cat%3Astat.ML%20OR%20cat%3Astat.ML%20OR%20cat%3Acs.LG%20OR%20cat%3Acs.MA%20OR%20cat%3Acs.NE%29%26id_list%3D%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=(ti:"Diffusion Models") AND (abs:"Diffusion Models") ANDNOT (abs:prostate AND abs:surgical AND abs:gender AND abs:ethinic AND abs:quadrotor) AND (cat:cs.AI OR cat:stat.ML OR cat:stat.ML OR cat:cs.LG OR cat:cs.MA OR cat:cs.NE)&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api//1IrZLJCDCBYbqcw1GKW7isQRxg</id>
  <updated>2025-02-16T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1719</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.09511v1</id>
    <updated>2025-02-13T17:22:50Z</updated>
    <published>2025-02-13T17:22:50Z</published>
    <title>Diffusion Models for Molecules: A Survey of Methods and Tasks</title>
    <summary>  Generative tasks about molecules, including but not limited to molecule
generation, are crucial for drug discovery and material design, and have
consistently attracted significant attention. In recent years, diffusion models
have emerged as an impressive class of deep generative models, sparking
extensive research and leading to numerous studies on their application to
molecular generative tasks. Despite the proliferation of related work, there
remains a notable lack of up-to-date and systematic surveys in this area.
Particularly, due to the diversity of diffusion model formulations, molecular
data modalities, and generative task types, the research landscape is
challenging to navigate, hindering understanding and limiting the area's
growth. To address this, this paper conducts a comprehensive survey of
diffusion model-based molecular generative methods. We systematically review
the research from the perspectives of methodological formulations, data
modalities, and task types, offering a novel taxonomy. This survey aims to
facilitate understanding and further flourishing development in this area. The
relevant papers are summarized at:
https://github.com/AzureLeon1/awesome-molecular-diffusion-models.
</summary>
    <author>
      <name>Liang Wang</name>
    </author>
    <author>
      <name>Chao Song</name>
    </author>
    <author>
      <name>Zhiyuan Liu</name>
    </author>
    <author>
      <name>Yu Rong</name>
    </author>
    <author>
      <name>Qiang Liu</name>
    </author>
    <author>
      <name>Shu Wu</name>
    </author>
    <author>
      <name>Liang Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2502.09511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.09511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.09151v1</id>
    <updated>2025-02-13T10:27:30Z</updated>
    <published>2025-02-13T10:27:30Z</published>
    <title>Regularization can make diffusion models more efficient</title>
    <summary>  Diffusion models are one of the key architectures of generative AI. Their
main drawback, however, is the computational costs. This study indicates that
the concept of sparsity, well known especially in statistics, can provide a
pathway to more efficient diffusion pipelines. Our mathematical guarantees
prove that sparsity can reduce the input dimension's influence on the
computational complexity to that of a much smaller intrinsic dimension of the
data. Our empirical findings confirm that inducing sparsity can indeed lead to
better samples at a lower cost.
</summary>
    <author>
      <name>Mahsa Taheri</name>
    </author>
    <author>
      <name>Johannes Lederer</name>
    </author>
    <link href="http://arxiv.org/abs/2502.09151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.09151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.08914v1</id>
    <updated>2025-02-13T03:05:42Z</updated>
    <published>2025-02-13T03:05:42Z</published>
    <title>Diffusion Models Through a Global Lens: Are They Culturally Inclusive?</title>
    <summary>  Text-to-image diffusion models have recently enabled the creation of visually
compelling, detailed images from textual prompts. However, their ability to
accurately represent various cultural nuances remains an open question. In our
work, we introduce CultDiff benchmark, evaluating state-of-the-art diffusion
models whether they can generate culturally specific images spanning ten
countries. We show that these models often fail to generate cultural artifacts
in architecture, clothing, and food, especially for underrepresented country
regions, by conducting a fine-grained analysis of different similarity aspects,
revealing significant disparities in cultural relevance, description fidelity,
and realism compared to real-world reference images. With the collected human
evaluations, we develop a neural-based image-image similarity metric, namely,
CultDiff-S, to predict human judgment on real and generated images with
cultural artifacts. Our work highlights the need for more inclusive generative
AI systems and equitable dataset representation over a wide range of cultures.
</summary>
    <author>
      <name>Zahra Bayramli</name>
    </author>
    <author>
      <name>Ayhan Suleymanzade</name>
    </author>
    <author>
      <name>Na Min An</name>
    </author>
    <author>
      <name>Huzama Ahmad</name>
    </author>
    <author>
      <name>Eunsu Kim</name>
    </author>
    <author>
      <name>Junyeong Park</name>
    </author>
    <author>
      <name>James Thorne</name>
    </author>
    <author>
      <name>Alice Oh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 17 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.08914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.08914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.08808v1</id>
    <updated>2025-02-12T21:44:06Z</updated>
    <published>2025-02-12T21:44:06Z</published>
    <title>A First-order Generative Bilevel Optimization Framework for Diffusion
  Models</title>
    <summary>  Diffusion models, which iteratively denoise data samples to synthesize
high-quality outputs, have achieved empirical success across domains. However,
optimizing these models for downstream tasks often involves nested bilevel
structures, such as tuning hyperparameters for fine-tuning tasks or noise
schedules in training dynamics, where traditional bilevel methods fail due to
the infinite-dimensional probability space and prohibitive sampling costs. We
formalize this challenge as a generative bilevel optimization problem and
address two key scenarios: (1) fine-tuning pre-trained models via an
inference-only lower-level solver paired with a sample-efficient gradient
estimator for the upper level, and (2) training diffusion models from scratch
with noise schedule optimization by reparameterizing the lower-level problem
and designing a computationally tractable gradient estimator. Our first-order
bilevel framework overcomes the incompatibility of conventional bilevel methods
with diffusion processes, offering theoretical grounding and computational
practicality. Experiments demonstrate that our method outperforms existing
fine-tuning and hyperparameter search baselines.
</summary>
    <author>
      <name>Quan Xiao</name>
    </author>
    <author>
      <name>Hui Yuan</name>
    </author>
    <author>
      <name>A F M Saif</name>
    </author>
    <author>
      <name>Gaowen Liu</name>
    </author>
    <author>
      <name>Ramana Kompella</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
    <author>
      <name>Tianyi Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2502.08808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.08808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.08598v1</id>
    <updated>2025-02-12T17:35:43Z</updated>
    <published>2025-02-12T17:35:43Z</published>
    <title>Enhancing Diffusion Models Efficiency by Disentangling Total-Variance
  and Signal-to-Noise Ratio</title>
    <summary>  The long sampling time of diffusion models remains a significant bottleneck,
which can be mitigated by reducing the number of diffusion time steps. However,
the quality of samples with fewer steps is highly dependent on the noise
schedule, i.e., the specific manner in which noise is introduced and the signal
is reduced at each step. Although prior work has improved upon the original
variance-preserving and variance-exploding schedules, these approaches
$\textit{passively}$ adjust the total variance, without direct control over it.
In this work, we propose a novel total-variance/signal-to-noise-ratio
disentangled (TV/SNR) framework, where TV and SNR can be controlled
independently. Our approach reveals that different existing schedules, where
the TV explodes exponentially, can be $\textit{improved}$ by setting a constant
TV schedule while preserving the same SNR schedule. Furthermore, generalizing
the SNR schedule of the optimal transport flow matching significantly improves
the performance in molecular structure generation, achieving few step
generation of stable molecules. A similar tendency is observed in image
generation, where our approach with a uniform diffusion time grid performs
comparably to the highly tailored EDM sampler.
</summary>
    <author>
      <name>Khaled Kahouli</name>
    </author>
    <author>
      <name>Winfried Ripken</name>
    </author>
    <author>
      <name>Stefan Gugler</name>
    </author>
    <author>
      <name>Oliver T. Unke</name>
    </author>
    <author>
      <name>Klaus-Robert MÃ¼ller</name>
    </author>
    <author>
      <name>Shinichi Nakajima</name>
    </author>
    <link href="http://arxiv.org/abs/2502.08598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.08598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.08488v1</id>
    <updated>2025-02-12T15:23:29Z</updated>
    <published>2025-02-12T15:23:29Z</published>
    <title>One-Shot Federated Learning with Classifier-Free Diffusion Models</title>
    <summary>  Federated learning (FL) enables collaborative learning without data
centralization but introduces significant communication costs due to multiple
communication rounds between clients and the server. One-shot federated
learning (OSFL) addresses this by forming a global model with a single
communication round, often relying on the server's model distillation or
auxiliary dataset generation - often through pre-trained diffusion models
(DMs). Existing DM-assisted OSFL methods, however, typically employ
classifier-guided DMs, which require training auxiliary classifier models at
each client, introducing additional computation overhead. This work introduces
OSCAR (One-Shot Federated Learning with Classifier-Free Diffusion Models), a
novel OSFL approach that eliminates the need for auxiliary models. OSCAR uses
foundation models to devise category-specific data representations at each
client, seamlessly integrated into a classifier-free diffusion model pipeline
for server-side data generation. OSCAR is a simple yet cost-effective OSFL
approach that outperforms the state-of-the-art on four benchmarking datasets
while reducing the communication load by at least 99%.
</summary>
    <author>
      <name>Obaidullah Zaland</name>
    </author>
    <author>
      <name>Shutong Jin</name>
    </author>
    <author>
      <name>Florian T. Pokorny</name>
    </author>
    <author>
      <name>Monowar Bhuyan</name>
    </author>
    <link href="http://arxiv.org/abs/2502.08488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.08488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>

["placeholder"]
